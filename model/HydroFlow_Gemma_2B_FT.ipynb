{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Fine-tuned Gemma 2B Model on Wastewater and Stromwater Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SDEExiAk4fLb",
    "papermill": {
     "duration": 0.00928,
     "end_time": "2024-02-21T09:37:33.77871",
     "exception": false,
     "start_time": "2024-02-21T09:37:33.76943",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "##### Copyright 2024 Google LLC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "papermill": {
     "duration": 0.016175,
     "end_time": "2024-02-21T09:37:33.803726",
     "exception": false,
     "start_time": "2024-02-21T09:37:33.787551",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZFWzQEqNosrS",
    "papermill": {
     "duration": 0.008399,
     "end_time": "2024-02-21T09:37:33.820666",
     "exception": false,
     "start_time": "2024-02-21T09:37:33.812267",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Fine-tune Gemma models in Keras using LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.008407,
     "end_time": "2024-02-21T09:37:33.837849",
     "exception": false,
     "start_time": "2024-02-21T09:37:33.829442",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://ai.google.dev/gemma/docs/lora_tuning\"><img src=\"https://ai.google.dev/static/site-assets/images/docs/notebook-site-button.png\" height=\"32\" width=\"32\" />View on ai.google.dev</a>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/google/generative-ai-docs/blob/main/site/en/gemma/docs/lora_tuning.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/335\"><img src=\"https://ai.google.dev/images/cloud-icon.svg\" width=\"40\" />Open in Vertex AI</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/google/generative-ai-docs/blob/main/site/en/gemma/docs/lora_tuning.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_kg_hide-output": true,
    "id": "lSGRSsRPgkzK",
    "papermill": {
     "duration": 0.008475,
     "end_time": "2024-02-21T09:37:33.854882",
     "exception": false,
     "start_time": "2024-02-21T09:37:33.846407",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Overview\n",
    "\n",
    "Gemma is a family of lightweight, state-of-the art open models built from the same research and technology used to create the Gemini models.\n",
    "\n",
    "Large Language Models (LLMs) like Gemma have been shown to be effective at a variety of NLP tasks. An LLM is first pre-trained on a large corpus of text in a self-supervised fashion. Pre-training helps LLMs learn general-purpose knowledge, such as statistical relationships between words. An LLM can then be fine-tuned with domain-specific data to perform downstream tasks (such as sentiment analysis).\n",
    "\n",
    "LLMs are extremely large in size (parameters in the order of millions). Full fine-tuning (which updates all the parameters in the model) is not required for most applications because typical fine-tuning datasets are relatively much smaller than the pre-training datasets.\n",
    "\n",
    "[Low Rank Adaptation (LoRA)](https://arxiv.org/abs/2106.09685){:.external} is a fine-tuning technique which greatly reduces the number of trainable parameters for downstream tasks by freezing the weights of the model and inserting a smaller number of new weights into the model. This makes training with LoRA much faster and more memory-efficient, and produces smaller model weights (a few hundred MBs), all while maintaining the quality of the model outputs.\n",
    "\n",
    "This tutorial walks you through using KerasNLP to perform LoRA fine-tuning on a Gemma 2B model using the [Wastewater and Stormwater Dataset](https://www.kaggle.com/datasets/databricks/databricks-dolly-15k){:.external}. This dataset contains 40,000 high-quality human-generated prompt / response pairs specifically designed for fine-tuning LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w1q6-W_mKIT-",
    "papermill": {
     "duration": 0.00852,
     "end_time": "2024-02-21T09:37:33.871909",
     "exception": false,
     "start_time": "2024-02-21T09:37:33.863389",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lyhHCMfoRZ_v",
    "papermill": {
     "duration": 0.009226,
     "end_time": "2024-02-21T09:37:33.889672",
     "exception": false,
     "start_time": "2024-02-21T09:37:33.880446",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Get access to Gemma\n",
    "\n",
    "To complete this tutorial, you will first need to complete the setup instructions at [Gemma setup](https://ai.google.dev/gemma/docs/setup). The Gemma setup instructions show you how to do the following:\n",
    "\n",
    "Gemma models are hosted by Kaggle. To use Gemma, request access on Kaggle:\n",
    "\n",
    "- Sign in or register at [kaggle.com](https://www.kaggle.com)\n",
    "- Open the [Gemma model card](https://www.kaggle.com/models/google/gemma) and select _\"Request Access\"_\n",
    "- Complete the consent form and accept the terms and conditions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m4enlpwvIu-3",
    "papermill": {
     "duration": 0.008687,
     "end_time": "2024-02-21T09:37:33.906835",
     "exception": false,
     "start_time": "2024-02-21T09:37:33.898148",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Install dependencies\n",
    "\n",
    "Install Keras, KerasNLP, and other dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "1eeBtYqJsZPG",
    "outputId": "d0645149-4fe7-4304-81dd-cb18354cd7c9",
    "papermill": {
     "duration": 29.215629,
     "end_time": "2024-02-21T09:38:03.131031",
     "exception": false,
     "start_time": "2024-02-21T09:37:33.915402",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install Keras 3 last. See https://keras.io/getting_started/ for more details.\n",
    "!pip install -q -U keras-nlp\n",
    "!pip install -q -U keras>=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Import json and pandas, include other libraries you may need.\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rGLS-l5TxIR4",
    "papermill": {
     "duration": 0.008909,
     "end_time": "2024-02-21T09:38:03.149611",
     "exception": false,
     "start_time": "2024-02-21T09:38:03.140702",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Select a backend\n",
    "\n",
    "Keras is a high-level, multi-framework deep learning API designed for simplicity and ease of use. Using Keras 3, you can run workflows on one of three backends: TensorFlow, JAX, or PyTorch.\n",
    "\n",
    "For this tutorial, configure the backend for JAX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "yn5uy8X8sdD0",
    "papermill": {
     "duration": 0.017153,
     "end_time": "2024-02-21T09:38:03.175604",
     "exception": false,
     "start_time": "2024-02-21T09:38:03.158451",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Connect with the kaggle using your username and key\n",
    "import os\n",
    "os.environ[\"KAGGLE_USERNAME\"] =  #kaggle username\n",
    "os.environ[\"KAGGLE_KEY\"] =  #kaggle key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting jax\n",
      "  Downloading jax-0.4.35-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting jaxlib\n",
      "  Downloading jaxlib-0.4.35-cp310-cp310-manylinux2014_x86_64.whl.metadata (983 bytes)\n",
      "Requirement already satisfied: ml-dtypes>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from jax) (0.4.1)\n",
      "Requirement already satisfied: numpy>=1.24 in /opt/conda/lib/python3.10/site-packages (from jax) (1.26.4)\n",
      "Requirement already satisfied: opt-einsum in /opt/conda/lib/python3.10/site-packages (from jax) (3.4.0)\n",
      "Requirement already satisfied: scipy>=1.10 in /opt/conda/lib/python3.10/site-packages (from jax) (1.13.1)\n",
      "Downloading jax-0.4.35-py3-none-any.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jaxlib-0.4.35-cp310-cp310-manylinux2014_x86_64.whl (87.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.3/87.3 MB\u001b[0m \u001b[31m110.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: jaxlib, jax\n",
      "Successfully installed jax-0.4.35 jaxlib-0.4.35\n"
     ]
    }
   ],
   "source": [
    "!pip install jax jaxlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hZs8XXqUKRmi",
    "papermill": {
     "duration": 0.008992,
     "end_time": "2024-02-21T09:38:03.193546",
     "exception": false,
     "start_time": "2024-02-21T09:38:03.184554",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Import packages\n",
    "\n",
    "Import Keras and KerasNLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "FYHyPUA9hKTf",
    "papermill": {
     "duration": 13.723138,
     "end_time": "2024-02-21T09:38:16.925885",
     "exception": false,
     "start_time": "2024-02-21T09:38:03.202747",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-15 21:34:21.717358: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-10-15 21:34:21.997434: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-10-15 21:34:22.282967: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-15 21:34:22.492180: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-15 21:34:22.552552: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-15 21:34:23.000770: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "#importing keras and kreas_nlp\n",
    "import keras\n",
    "import keras_nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9T7xe_jzslv4",
    "papermill": {
     "duration": 0.008653,
     "end_time": "2024-02-21T09:38:16.943901",
     "exception": false,
     "start_time": "2024-02-21T09:38:16.935248",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Load Dataset and Preprocess (as needed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "45UpBDfBgf0I",
    "papermill": {
     "duration": 0.008546,
     "end_time": "2024-02-21T09:38:16.961234",
     "exception": false,
     "start_time": "2024-02-21T09:38:16.952688",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Preprocess the data. This fine-tuning uses a subset of 2000 training examples to execute the notebook faster. More or less may be useful depending on your scenerio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instruction</th>\n",
       "      <th>context</th>\n",
       "      <th>response</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When is the next maintenance due for Lift Stat...</td>\n",
       "      <td>On 1784-01-12 06:33:12, technician Michael Geo...</td>\n",
       "      <td>Based on the last service date of 1784-01-12 a...</td>\n",
       "      <td>closed_qa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Based on the lifespan of Sludge Thickener, how...</td>\n",
       "      <td>On 1753-05-24 10:11:01, technician Virginia Sa...</td>\n",
       "      <td>The Sludge Thickener-487731 was installed on 1...</td>\n",
       "      <td>closed_qa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>When is the next maintenance due for Reverse O...</td>\n",
       "      <td>On 1863-10-05 06:15:22, technician Henry Woods...</td>\n",
       "      <td>Based on the last service date of 1863-10-05 a...</td>\n",
       "      <td>closed_qa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What kind of maintenance would be needed for R...</td>\n",
       "      <td>On 1892-11-17 01:47:25, technician Miranda Hay...</td>\n",
       "      <td>Given that the Reverse Osmosis System-50577 is...</td>\n",
       "      <td>closed_qa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What maintenance action was performed on the M...</td>\n",
       "      <td>On 2002-06-03 14:53:52, technician Ashley Mcbr...</td>\n",
       "      <td>A emergency repair was performed on Membrane B...</td>\n",
       "      <td>closed_qa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What maintenance action was performed on the A...</td>\n",
       "      <td>On 1994-11-12 12:35:54, technician Amanda Guti...</td>\n",
       "      <td>A emergency repair was performed on Activated ...</td>\n",
       "      <td>closed_qa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What kind of maintenance would be needed for G...</td>\n",
       "      <td>On 1709-09-29 08:32:03, technician Timothy Cra...</td>\n",
       "      <td>Given that the Grit Chamber-581333 is currentl...</td>\n",
       "      <td>closed_qa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What maintenance action was performed on the M...</td>\n",
       "      <td>On 1998-11-22 13:34:09, technician Kathleen Cl...</td>\n",
       "      <td>A emergency repair was performed on Membrane B...</td>\n",
       "      <td>closed_qa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>When is the next maintenance due for Primary C...</td>\n",
       "      <td>On 1777-04-04 10:31:34, technician Michael Bri...</td>\n",
       "      <td>Based on the last service date of 1777-04-04 a...</td>\n",
       "      <td>closed_qa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>When is the next maintenance due for Sludge Th...</td>\n",
       "      <td>On 1834-12-24 17:39:33, technician Christina R...</td>\n",
       "      <td>Based on the last service date of 1834-12-24 a...</td>\n",
       "      <td>closed_qa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         instruction  \\\n",
       "0  When is the next maintenance due for Lift Stat...   \n",
       "1  Based on the lifespan of Sludge Thickener, how...   \n",
       "2  When is the next maintenance due for Reverse O...   \n",
       "3  What kind of maintenance would be needed for R...   \n",
       "4  What maintenance action was performed on the M...   \n",
       "5  What maintenance action was performed on the A...   \n",
       "6  What kind of maintenance would be needed for G...   \n",
       "7  What maintenance action was performed on the M...   \n",
       "8  When is the next maintenance due for Primary C...   \n",
       "9  When is the next maintenance due for Sludge Th...   \n",
       "\n",
       "                                             context  \\\n",
       "0  On 1784-01-12 06:33:12, technician Michael Geo...   \n",
       "1  On 1753-05-24 10:11:01, technician Virginia Sa...   \n",
       "2  On 1863-10-05 06:15:22, technician Henry Woods...   \n",
       "3  On 1892-11-17 01:47:25, technician Miranda Hay...   \n",
       "4  On 2002-06-03 14:53:52, technician Ashley Mcbr...   \n",
       "5  On 1994-11-12 12:35:54, technician Amanda Guti...   \n",
       "6  On 1709-09-29 08:32:03, technician Timothy Cra...   \n",
       "7  On 1998-11-22 13:34:09, technician Kathleen Cl...   \n",
       "8  On 1777-04-04 10:31:34, technician Michael Bri...   \n",
       "9  On 1834-12-24 17:39:33, technician Christina R...   \n",
       "\n",
       "                                            response   category  \n",
       "0  Based on the last service date of 1784-01-12 a...  closed_qa  \n",
       "1  The Sludge Thickener-487731 was installed on 1...  closed_qa  \n",
       "2  Based on the last service date of 1863-10-05 a...  closed_qa  \n",
       "3  Given that the Reverse Osmosis System-50577 is...  closed_qa  \n",
       "4  A emergency repair was performed on Membrane B...  closed_qa  \n",
       "5  A emergency repair was performed on Activated ...  closed_qa  \n",
       "6  Given that the Grit Chamber-581333 is currentl...  closed_qa  \n",
       "7  A emergency repair was performed on Membrane B...  closed_qa  \n",
       "8  Based on the last service date of 1777-04-04 a...  closed_qa  \n",
       "9  Based on the last service date of 1834-12-24 a...  closed_qa  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Examples of the Dataset\n",
    "data_list = []\n",
    "file_path = \"llm_merged_data.jsonl\"\n",
    "\n",
    "with open(file_path, 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i < 10:  # Only process the first two lines\n",
    "            data = json.loads(line)\n",
    "            data_list.append(data)\n",
    "        else:\n",
    "            break  # Stop reading after two lines\n",
    "\n",
    "# Convert to DataFrame and display the first two rows\n",
    "df = pd.DataFrame(data_list)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "data = []  # Initialize the empty list to save the data\n",
    "seen = set()  # Set to track unique observations\n",
    "\n",
    "with open(\"llm_merged_data.jsonl\") as file:\n",
    "    for line in file:\n",
    "        features = json.loads(line)\n",
    "        unique_key = (features['instruction'], features['response'])\n",
    "        if unique_key not in seen:\n",
    "            seen.add(unique_key)\n",
    "            template = (\n",
    "                \"Instruction: {instruction}\\n\\n\"\n",
    "                \"Response: {response}\\n\"\n",
    "            ).format(**features)\n",
    "            data.append(template)\n",
    "            \n",
    "            if len(data) == 2000:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the dataset to be used for training is: 2000\n"
     ]
    }
   ],
   "source": [
    "length = len(data)\n",
    "print(f\"The length of the dataset to be used for training is: {length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Based on the lifespan of Lift Station, how much longer can Lift Station-153125 be expected to operate?\n",
      "\n",
      "Response: The Lift Station-153125 was installed on 1973-09-25 and has been in operation for approximately -30.7 years. Given the typical lifespan of 25 years for Lift Station, it can be expected to operate for about 55.7 more years, assuming proper maintenance. However, its performance should be closely monitored as it approaches the end of its expected lifespan.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the first item in the list for inspection\n",
    "print(data[0] if data else \"No unique data found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "# Check if the list is empty before accessing elements and then checing the data type\n",
    "if data:\n",
    "  print(type(data[0]))\n",
    "\n",
    "  # If the data type is not string, convert it\n",
    "  if not isinstance(data[0], str):\n",
    "    # Example: Convert numerical data to string\n",
    "    data = [str(x) for x in data]\n",
    "else:\n",
    "  print(\"The data list is empty.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to create a prompt dynamically\n",
    "def create_prompt(instruction):\n",
    "    # Create a generic prompt structure\n",
    "    prompt = (\n",
    "        \"Instruction: {instruction}\\n\"\n",
    "        \"Response:\"\n",
    "    ).format(instruction=instruction)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7RCE3fdGhDE5",
    "papermill": {
     "duration": 0.008892,
     "end_time": "2024-02-21T09:38:17.316544",
     "exception": false,
     "start_time": "2024-02-21T09:38:17.307652",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Load Model\n",
    "\n",
    "KerasNLP provides implementations of many popular [model architectures](https://keras.io/api/keras_nlp/models/){:.external}. In this tutorial, you'll create a model using `GemmaCausalLM`, an end-to-end Gemma model for causal language modeling. A causal language model predicts the next token based on previous tokens.\n",
    "\n",
    "Create the model using the `from_preset` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "vz5zLEyLstfn",
    "outputId": "51cc6fc3-e1bd-4a5c-dff7-4425debbd4e2",
    "papermill": {
     "duration": 53.182225,
     "end_time": "2024-02-21T09:39:10.507854",
     "exception": false,
     "start_time": "2024-02-21T09:38:17.325629",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                                                  </span>┃<span style=\"font-weight: bold\">                                   Config </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                              │                      Vocab size: <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n",
       "└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                                                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                  Config\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                              │                      Vocab size: \u001b[38;5;34m256,000\u001b[0m │\n",
       "└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">524,288,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)        │   \u001b[38;5;34m2,506,172,416\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
       "│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m524,288,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> (9.34 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,506,172,416\u001b[0m (9.34 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> (9.34 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,506,172,416\u001b[0m (9.34 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma_2b_en\")\n",
    "gemma_lm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nl4lvPy5zA26",
    "papermill": {
     "duration": 0.012623,
     "end_time": "2024-02-21T09:39:10.53129",
     "exception": false,
     "start_time": "2024-02-21T09:39:10.518667",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The `from_preset` method instantiates the model from a preset architecture and weights. In the code above, the string \"gemma_2b_en\" specifies the preset architecture — a Gemma model with 2 billion parameters.\n",
    "\n",
    "NOTE: A Gemma model with 7\n",
    "billion parameters is also available. To run the larger model in Colab, you need access to the premium GPUs available in paid plans. Alternatively, you can perform [distributed tuning on a Gemma 7B model](https://ai.google.dev/gemma/docs/distributed_tuning) on Kaggle or Google Cloud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G_L6A5J-1QgC",
    "papermill": {
     "duration": 0.010947,
     "end_time": "2024-02-21T09:39:10.55264",
     "exception": false,
     "start_time": "2024-02-21T09:39:10.541693",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Inference before fine tuning\n",
    "\n",
    "In this section, you will query the model with various prompts to see how it responds.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PVLXadptyo34",
    "papermill": {
     "duration": 0.010445,
     "end_time": "2024-02-21T09:39:10.573368",
     "exception": false,
     "start_time": "2024-02-21T09:39:10.562923",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Reverse Osmosis System Status and maintenance\n",
    "\n",
    "Query the model for suggestions on what to do on the aaration tank based on the status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "ZwQz3xxxKciD",
    "outputId": "a5b9a594-a4c4-4768-d5fe-a41e527114b1",
    "papermill": {
     "duration": 16.93148,
     "end_time": "2024-02-21T09:39:27.515282",
     "exception": false,
     "start_time": "2024-02-21T09:39:10.583802",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1729028159.816639  164891 service.cc:146] XLA service 0x7f07f40375f0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1729028159.816801  164891 service.cc:154]   StreamExecutor device (0): Host, Default Version\n",
      "I0000 00:00:1729028159.953926  164891 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: When is the next maintenance due for Reverse Osmosis System-683384 based on its last service date?\n",
      "Response: The next maintenance due date is 12/31/2020.\n",
      "Instruction: When is the next maintenance due for Reverse Osmosis System-683384 based on its last service date?\n",
      "Response: The next maintenance due date is 12/31/2020.\n",
      "Instruction: When is the next maintenance due for Reverse Osmosis System-683384 based on its last service date?\n",
      "Response: The next maintenance due date is 12/31/2020.\n",
      "Instruction: When is the next maintenance due for Reverse Osmosis System-683384 based on its last service date?\n",
      "Response: The next maintenance due date is 12/31/2020.\n",
      "Instruction: When is the next maintenance due for Reverse Osmosis System-683384 based on its last service date?\n",
      "Response: The next maintenance due date is 12/31/2020.\n",
      "Instruction: When is the next maintenance due for Reverse Osmosis System-683384 based on its last service date?\n",
      "Response: The next maintenance due date is 12/31/2020.\n",
      "Instruction: When is the next maintenance due for Reverse Osmosis System-683384 based on its last service date?\n",
      "Response: The next maintenance due date is 12/31/2020.\n",
      "Instruction: When is the next maintenance due for Reverse Osmosis System-683384 based on its last service date?\n",
      "Response: The next maintenance due date is 12/31/2020.\n",
      "Instruction: When is the next maintenance due for Reverse Osmosis System-683384 based on its last service date?\n",
      "Response: The next maintenance due date is 12/31/2020.\n",
      "Instruction: When is the next maintenance due for Reverse Osmosis System-683384 based on its last service date?\n",
      "Response: The next maintenance due date is 12/31/2020.\n",
      "Instruction: When is the next maintenance due for Reverse Osmosis System-683384 based on its last service date?\n",
      "Response: The next maintenance due date is 12/31/2020.\n",
      "Instruction: When is the next maintenance due for Reverse Osmosis System-683384 based on its last service date?\n",
      "Response: The next maintenance due date is 12/31/2020.\n",
      "Instruction: When is the next maintenance due for Reverse Osmosis System-683384 based on its last service date?\n",
      "Response: The next maintenance due date is 12/31/2020.\n",
      "Instruction: When is the next maintenance due for Reverse Osmosis System-683384 based on its last service date?\n",
      "Response: The next maintenance due date is 12/31/2020.\n",
      "Instruction: When is the next maintenance due for Reverse Osmosis System-683384 based on its last service date?\n",
      "Response: The next maintenance due date is 12/31/2020.\n",
      "Instruction: When is the next maintenance due for Reverse Osmosis System-683384 based on its last service date?\n",
      "Response: The next maintenance due date is 12/31/2020.\n",
      "Instruction: When is the next maintenance due for Reverse Osmosis System-683384 based on its last service date?\n",
      "Response: The next maintenance due date is 12/31/2020.\n",
      "Instruction: When is the next maintenance due for Reverse Osmosis System-683384 based on its last service date?\n",
      "Response: The next maintenance due date is 12/31/2020.\n",
      "Instruction: When is the next maintenance due for Reverse Osmosis System-683384 based on its last service date?\n",
      "Response: The next maintenance due date is 12/31/2020.\n",
      "Instruction: When is the next maintenance due for Reverse Osmosis System-683384 based on its last service date?\n",
      "Response: The next maintenance due date is 12/31/2020.\n",
      "Instruction: When is the next maintenance due for Reverse Osmosis System-683384 based on its last service date?\n",
      "Response: The next maintenance due date is 12/31/\n"
     ]
    }
   ],
   "source": [
    "#trying the pre-trianed gemma model on my dataset.\n",
    "prompt = create_prompt(\"When is the next maintenance due for Reverse Osmosis System-683384 based on its last service date?\",\n",
    ")\n",
    "print(gemma_lm.generate(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AePQUIs2h-Ks",
    "papermill": {
     "duration": 0.010616,
     "end_time": "2024-02-21T09:39:27.537776",
     "exception": false,
     "start_time": "2024-02-21T09:39:27.52716",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The model just responds with a recommendation of when the equipment will be maintained next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YQ74Zz_S0iVv",
    "papermill": {
     "duration": 0.010188,
     "end_time": "2024-02-21T09:39:27.558437",
     "exception": false,
     "start_time": "2024-02-21T09:39:27.548249",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Grit Chamber-392614\n",
    "\n",
    "Prompt the model to explain the parts that were used during the maintenance of Grit Chamber-392614.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Instruction: What kind of maintenance would be needed for Grit Chamber-3926147\n",
      "Response:\n",
      "Grit Chamber-3926147\n",
      "1. The grit chamber is a kind of equipment for removing the sand and gravel in the water. It is mainly used for the treatment of water with high sand content. The sand and gravel in the water are removed by the sand and gravel removal device. The sand and gravel removal device is composed of a sand and gravel removal device and a sand and gravel removal device. The sand and gravel removal device is composed of a sand and gravel removal device and a sand and gravel removal device. The sand and gravel removal device is composed of a sand and gravel removal device and a sand and gravel removal device. The sand and gravel removal device is composed of a sand and gravel removal device and a sand and gravel removal device. The sand and gravel removal device is composed of a sand and gravel removal device and a sand and gravel removal device. The sand and gravel removal device is composed of a sand and gravel removal device and a sand and gravel removal device. The sand and gravel removal device is composed of a sand and gravel removal device and a sand and gravel removal device. The sand and gravel removal device is composed of a sand and gravel removal device and a sand and gravel removal device. The sand and gravel removal device is composed of a sand and gravel removal device and a sand and gravel removal device. The sand and gravel removal device is composed of a sand and gravel removal device and a sand and gravel removal device. The sand and gravel removal device is composed of a sand and gravel removal device and a sand and gravel removal device. The sand and gravel removal device is composed of a sand and gravel removal device and a sand and gravel removal device. The sand and gravel removal device is composed of a sand and gravel removal device and a sand and gravel removal device. The sand and gravel removal device is composed of a sand and gravel removal device and a sand and gravel removal device. The sand and gravel removal device is composed of a sand and gravel removal device and a sand and gravel removal device. The sand and gravel removal device is composed of a sand and gravel removal device and a sand and gravel removal device. The sand and gravel removal device is composed of a sand and gravel removal device and a sand and gravel removal device. The sand and gravel removal device is composed of a sand and gravel removal device and a sand and gravel removal device. The sand and gravel removal device is composed of a sand and gravel removal device and a sand and gravel removal device. The sand and gravel removal device is composed of a sand and gravel removal device and a sand and gravel removal device. The sand and gravel removal device is composed of a sand and gravel removal device and a sand and gravel removal device. The sand and gravel removal device is composed of a sand and gravel removal device and a sand and gravel removal device. The sand and gravel removal device is composed of a sand and gravel removal device and a sand and gravel removal device. The sand and gravel removal device is composed of a sand and gravel removal device and a sand and gravel removal device. The sand and gravel removal device is composed of a sand and gravel removal device and a sand and gravel removal device. The sand and gravel removal device is composed of a sand and gravel removal device and a sand and gravel removal device. The sand and gravel removal device is composed of a sand and gravel removal device and a sand and gravel removal device. The sand and gravel removal device is composed of a sand and gravel removal device and a sand and gravel removal device. The sand and gravel removal device is composed of a sand and gravel removal device and a sand and gravel removal device. The sand and gravel removal device is composed of a sand and gravel removal device and a sand and gravel removal device. The sand and gravel removal device is composed of a sand and gravel removal device and a sand and gravel removal device. The sand and gravel removal device is composed of a sand and gravel removal device and a sand and gravel removal device. The sand and gravel removal device is composed of a sand and gravel removal device and a sand and gravel removal device. The sand and gravel removal device is composed of a sand and gravel removal device and a sand and gravel removal device. The sand and gravel removal device is composed of a sand and gravel removal device and a sand and gravel removal device. The sand and gravel removal device is composed of a sand and gravel removal device and a sand and gravel removal device. The sand and gravel removal device is composed of a sand and gravel removal device and a sand and gravel removal device. The sand and gravel removal device is composed of a sand and gravel removal device and a sand and gravel removal device. The sand and gravel removal device is composed of a sand and gravel removal device and a sand and gravel removal device. The sand and gravel removal device is composed of a sand and gravel removal device and a sand and gravel removal device. The sand and gravel removal device is composed of a sand and gravel removal device\n"
     ]
    }
   ],
   "source": [
    "#same thing for grit chamber\n",
    "prompt = create_prompt(\"Instruction: What kind of maintenance would be needed for Grit Chamber-3926147\")\n",
    "print(gemma_lm.generate(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pt7Nr6a7tItO",
    "papermill": {
     "duration": 0.01047,
     "end_time": "2024-02-21T09:39:33.354485",
     "exception": false,
     "start_time": "2024-02-21T09:39:33.344015",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## LoRA Fine-tuning\n",
    "\n",
    "To get better responses from the model, fine-tune the model with Low Rank Adaptation (LoRA) using a wastewater and stormwwater dataset.\n",
    "\n",
    "The LoRA rank determines the dimensionality of the trainable matrices that are added to the original weights of the LLM. It controls the expressiveness and precision of the fine-tuning adjustments.\n",
    "\n",
    "A higher rank means more detailed changes are possible, but also means more trainable parameters. A lower rank means less computational overhead, but potentially less precise adaptation.\n",
    "\n",
    "This tutorial uses a LoRA rank of 4. In practice, begin with a relatively small rank (such as 4, 8, 16). This is computationally efficient for experimentation. Train your model with this rank and evaluate the performance improvement on your task. Gradually increase the rank in subsequent trials and see if that further boosts performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "RCucu6oHz53G",
    "outputId": "0d8c80d7-0ab5-4fd3-e219-b2df4464084c",
    "papermill": {
     "duration": 0.511035,
     "end_time": "2024-02-21T09:39:33.876166",
     "exception": false,
     "start_time": "2024-02-21T09:39:33.365131",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                                                  </span>┃<span style=\"font-weight: bold\">                                   Config </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                              │                      Vocab size: <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n",
       "└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                                                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                  Config\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                              │                      Vocab size: \u001b[38;5;34m256,000\u001b[0m │\n",
       "└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,508,218,368</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">524,288,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)        │   \u001b[38;5;34m2,508,218,368\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
       "│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m524,288,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,508,218,368</span> (9.34 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,508,218,368\u001b[0m (9.34 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,045,952</span> (7.80 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,045,952\u001b[0m (7.80 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> (9.34 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,506,172,416\u001b[0m (9.34 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Enable LoRA for the model and set the LoRA rank to 5.\n",
    "gemma_lm.backbone.enable_lora(rank=6)\n",
    "gemma_lm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hQQ47kcdpbZ9",
    "papermill": {
     "duration": 0.011797,
     "end_time": "2024-02-21T09:39:33.903795",
     "exception": false,
     "start_time": "2024-02-21T09:39:33.891998",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Note that enabling LoRA reduces the number of trainable parameters significantly (from 2.5 billion to 1.3 million)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Limit the input sequence length to 512 (to control memory usage).\n",
    "from keras_nlp.samplers import TopKSampler\n",
    "sampler = TopKSampler(temperature=0.7, k=50)\n",
    "\n",
    "gemma_lm.preprocessor.sequence_length = 512\n",
    "# Use AdamW (a common optimizer for transformer models).\n",
    "optimizer = keras.optimizers.AdamW(\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "# Exclude layernorm and bias terms from decay.\n",
    "optimizer.exclude_from_weight_decay(var_names=[\"bias\", \"scale\"])\n",
    "\n",
    "gemma_lm.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=optimizer,\n",
    "    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "    sampler = sampler,\n",
    ")\n",
    "gemma_lm.fit(data, epochs=1, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Saving the model (can be saved in different formats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the finetuned model as a KerasNLP preset.\n",
    "gemma_lm.save_to_preset(gemma-HydroSense-instruct_2b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# #saving the model\n",
    "gemma_lm.save(\"gemma_model_updated.keras\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4yd-1cNw1dTn",
    "papermill": {
     "duration": 0.092253,
     "end_time": "2024-02-21T09:52:07.553072",
     "exception": false,
     "start_time": "2024-02-21T09:52:07.460819",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Inference after fine-tuning\n",
    "After fine-tuning, responses follow the instruction provided in the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to create a prompt based on any question\n",
    "def create_prompt(instruction):\n",
    "    # Create a generic prompt structure\n",
    "    prompt = (\n",
    "        \"Instruction: {instruction}\\n\"\n",
    "        \"Response:\"\n",
    "    ).format(instruction=instruction)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: What kind of maintenance would be needed for Lift Station-48560 given its current status?\n",
      "\n",
      "Response: Given that the Lift Station-48560 is currently operational, it would likely need routine maintenance to ensure continued optimal performance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example question\n",
    "user_question = \"What kind of maintenance would be needed for Lift Station-48560 given its current status?\"\n",
    "\n",
    "# Create the prompt\n",
    "prompt = create_prompt(user_question)\n",
    "\n",
    "# Generate the response\n",
    "generated_response = gemma_lm.generate(prompt)  # Adjust based on your model's method\n",
    "\n",
    "# Print the output\n",
    "print(f\"Model Answer: {generated_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Answer: Instruction: hat kind of maintenance would be needed for Reverse Osmosis System-50577?\n",
      "Response: Reverse Osmosis System-50577 needs routine maintenance.\n",
      "\n",
      "Instruction: how often should Reverse Osmosis System-50577 be cleaned and maintained?\n",
      "Response: Reverse Osmosis System-50577 should be cleaned and maintained every 6 months.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example question\n",
    "user_question = \"what kind of maintenance would be needed for Reverse Osmosis System-50577?\"\n",
    "\n",
    "# Create the prompt\n",
    "prompt = create_prompt(user_question)\n",
    "\n",
    "# Generate the response\n",
    "generated_response = gemma_lm.generate(prompt)  # Adjust based on your model's method\n",
    "\n",
    "# Print the output\n",
    "print(f\"Model Answer: {generated_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rtj1MWG2HBPv",
    "papermill": {
     "duration": 0.094695,
     "end_time": "2024-02-21T09:52:24.685134",
     "exception": false,
     "start_time": "2024-02-21T09:52:24.590439",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Note that for demonstration purposes, this tutorial fine-tunes the model on a small subset of the dataset for just one epoch and with a low LoRA rank value. To get better responses from the fine-tuned model based on dataset, you can experiment with:\n",
    "\n",
    "1. Increasing the size of the fine-tuning dataset\n",
    "2. Training for more steps (epochs)\n",
    "3. Setting a higher LoRA rank\n",
    "4. Modifying the hyperparameter values such as `learning_rate` and `weight_decay`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gSsRdeiof_rJ",
    "papermill": {
     "duration": 0.091976,
     "end_time": "2024-02-21T09:52:24.869733",
     "exception": false,
     "start_time": "2024-02-21T09:52:24.777757",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Next steps and Additional Resources\n",
    "\n",
    "This tutorial covered LoRA fine-tuning on a Gemma model using KerasNLP. Check out the following docs next:\n",
    "\n",
    "* Learn how to [generate text with a Gemma model](https://ai.google.dev/gemma/docs/get_started).\n",
    "* Learn how to perform [distributed fine-tuning and inference on a Gemma model](https://ai.google.dev/gemma/docs/distributed_tuning).\n",
    "* Learn how to [use Gemma open models with Vertex AI](https://cloud.google.com/vertex-ai/docs/generative-ai/open-models/use-gemma).\n",
    "* Learn how to [fine-tune Gemma using KerasNLP and deploy to Vertex AI](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_gemma_kerasnlp_to_vertexai.ipynb).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uploading the model to Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the preset as a new model variant on Kaggle\n",
    "kaggle_uri = \"kaggle://username/gemma-pirate/keras/gemma-HydroSense-instruct-2b\" #url to your kaggle and the name of the model\n",
    "keras_nlp.upload_preset(kaggle_uri, \"./gemma-HydroSense-instruct-2b\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m125"
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 3122881,
     "sourceId": 5385487,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 3533,
     "modelInstanceId": 5171,
     "sourceId": 10260,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 897.635437,
   "end_time": "2024-02-21T09:52:28.59121",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-02-21T09:37:30.955773",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
